{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=np.random.randint(low=1,high=20,size=20000)\n",
    "\n",
    "\n",
    "x2=np.random.randint(low=1,high=20,size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=3+2*x1-4*x2+np.random.random(20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that we have generated data such that y is an approximate linear combination of x1 and x2, next we'll calculate optimal parameter values using gradient descent and compare them with results from sklearn and we'll see how good is the method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame({'intercept':np.ones(x1.shape[0]),'x1':x1,'x2':x2})\n",
    "\n",
    "w=np.random.random(x.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-56.09137984, -30.66773532, -50.67174653, ...,  11.62136621,\n",
       "       -20.02142877,   5.09106376])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets write functions for predictions, error, cost and gradient that we discussed above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([13.18182866, 11.93584716, 12.64021729, ...,  4.84156431,\n",
       "       11.58366209,  6.2089265 ])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myprediction(features,weights):\n",
    "\n",
    "    predictions=np.dot(features,weights)\n",
    "    return(predictions)\n",
    "\n",
    "myprediction(x,w)\n",
    "\n",
    "# function numpy.dot : is used for matrix multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that , `np.dot` here is being used for matrix multiplication . Simple multiplication results to element wise multiplication , which is simply wrong in this context ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-69.2732085 , -42.60358248, -63.31196383, ...,   6.77980189,\n",
       "       -31.60509087,  -1.11786275])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def myerror(target,features,weights):\n",
    "    error=target-myprediction(features,weights)\n",
    "    return(error)\n",
    "myerror(y,x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26988672.160963915"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mycost(target,features,weights):\n",
    "    error=myerror(target,features,weights)\n",
    "    cost=np.dot(error.T,error)\n",
    "    return(cost)\n",
    "\n",
    "mycost(y,x,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 24.36623849, 184.45321497, 383.48750706])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gradient(target,features,weights):\n",
    "    \n",
    "    error=myerror(target,features,weights)\n",
    "    \n",
    "    gradient=-np.dot(features.T,error)/features.shape[0]\n",
    "    \n",
    "    return(gradient)\n",
    "\n",
    "gradient(y,x,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that gradient here is vector of 3 values because there are 3 parameters . Also since this is being evaluated on the entire data, we scaled it down with number of observations . Do recall that , the approximation which led to the ultimate results was that change in parameters is small. We dont have any direct control over gradient , we can always chose a small value for $\\eta$ to ensure that change in parameter remains small. Also if we end up chosing too small value for $\\eta$, we'll need to take larger number of steps to change in parameter in order to arrive at the optimal value of the parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets looks at the expected value for parameters from sklearn . Dont worry about the syntax here , we'll discuss that in detail, when we formally start with linear models in next module ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(x.iloc[:,1:],y)\n",
    "sk_estimates=([lr.intercept_]+list(lr.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.497047463340305, 2.0000507703755974, -3.999730754941359]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the same , these might be different for you, as we generated the data randomly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets write our version of this , using gradient descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_lr(target,features,learning_rate,num_steps,print_when):\n",
    "    \n",
    "    # start with random values of parameters\n",
    "    weights=np.random.random(features.shape[1])\n",
    "    \n",
    "    # change parameter multiple times in sequence \n",
    "    # using the cost function gradient which we discussed earlier \n",
    "    for i in range(num_steps):\n",
    "        \n",
    "        weights =weights- learning_rate*gradient(target,features,weights)\n",
    "       \n",
    "    # this simply prints the cost function value every (print_when)th iteration\n",
    "        if i%print_when==0:\n",
    "            print(mycost(target,features,weights),weights)\n",
    "        \n",
    "    return(weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37663827.46206986 [0.22601932 0.86074853 0.83003571]\n",
      "27010820.13853439 [0.19555256 0.59641581 0.38580932]\n",
      "20124049.214374103 [0.17149693 0.40269982 0.01810048]\n",
      "15614321.66494527 [ 0.15252241  0.26448127 -0.28850124]\n",
      "12608404.998134563 [ 0.1375749   0.16979156 -0.54622288]\n",
      "10557184.76876914 [ 0.125819    0.10915866 -0.76476819]\n",
      "9115164.508575158 [ 0.11659262  0.07508881 -0.95184064]\n",
      "8064776.524146876 [ 0.10937104  0.06165565 -1.11355813]\n",
      "7268838.880803236 [ 0.10373838  0.0641747  -1.2547815 ]\n",
      "6640691.346660266 [ 0.09936507  0.07894531 -1.37937498]\n",
      "6125435.742959881 [ 0.09598987  0.10304623 -1.49041263]\n",
      "5688151.164908491 [ 0.09340578  0.13417351 -1.5903419 ]\n",
      "5306490.27288684 [ 0.09144872  0.17051216 -1.6811133 ]\n",
      "4966027.461901615 [ 0.08998865  0.21063434 -1.76428321]\n",
      "4657335.595387895 [ 0.08892251  0.25341872 -1.84109529]\n",
      "4374148.546691798 [ 0.08816863  0.29798662 -1.91254512]\n",
      "4112205.822855315 [ 0.08766227  0.34365134 -1.9794313 ]\n",
      "3868525.6830490814 [ 0.08735212  0.38987804 -2.04239607]\n",
      "3640947.466954057 [ 0.08719751  0.436252   -2.1019575 ]\n",
      "3427843.081074387 [ 0.08716623  0.48245336 -2.15853492]\n",
      "3227934.795714404 [ 0.08723272  0.52823722 -2.21246919]\n",
      "3040179.8741729036 [ 0.08737673  0.57341784 -2.26403873]\n",
      "2863697.234057608 [ 0.08758223  0.61785609 -2.31347218]\n",
      "2697720.5604074458 [ 0.08783649  0.66144956 -2.36095856]\n",
      "2541568.0815155315 [ 0.08812942  0.7041247  -2.40665519]\n",
      "2394622.8560224846 [ 0.08845304  0.74583059 -2.4506941 ]\n",
      "2256319.704867737 [ 0.08880102  0.78653407 -2.49318703]\n",
      "2126136.3570795683 [ 0.08916832  0.82621581 -2.53422941]\n",
      "2003587.2801121532 [ 0.08955098  0.86486724 -2.57390359]\n",
      "1888219.2319576954 [ 0.08994582  0.9024881  -2.61228132]\n",
      "1779607.928226583 [ 0.09035035  0.93908456 -2.64942575]\n",
      "1677355.4410947321 [ 0.09076259  0.97466767 -2.68539304]\n",
      "1581088.0876436043 [ 0.09118096  1.00925217 -2.72023366]\n",
      "1490454.6535549534 [ 0.09160421  1.04285555 -2.75399333]\n",
      "1405124.853771667 [ 0.09203135  1.0754973  -2.78671388]\n",
      "1324787.9667856044 [ 0.0924616   1.10719833 -2.81843387]\n",
      "1249151.6013183983 [ 0.09289432  1.13798052 -2.84918914]\n",
      "1177940.568130311 [ 0.09332903  1.16786637 -2.87901317]\n",
      "1110895.8385463431 [ 0.09376534  1.19687867 -2.90793746]\n",
      "1047773.5769252962 [ 0.09420292  1.22504036 -2.93599176]\n",
      "988344.2379086246 [ 0.09464153  1.25237432 -2.96320432]\n",
      "932391.7216208318 [ 0.09508097  1.27890322 -2.98960206]\n",
      "879712.5815224333 [ 0.09552109  1.30464947 -3.01521068]\n",
      "830115.280636223 [ 0.09596176  1.32963513 -3.04005481]\n",
      "783419.4925636906 [ 0.09640287  1.35388183 -3.06415811]\n",
      "739455.4441980177 [ 0.09684435  1.37741078 -3.08754333]\n",
      "698063.2973969894 [ 0.09728613  1.4002427  -3.1102324 ]\n",
      "659092.5671496817 [ 0.09772817  1.42239782 -3.13224646]\n",
      "622401.5739844921 [ 0.09817041  1.44389586 -3.15360592]\n",
      "587856.9285414307 [ 0.09861283  1.46475605 -3.17433054]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.09901114,  1.48300032, -3.19245568])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lr(y,x,.0001,500,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.497047463340305, 2.0000507703755974, -3.999730754941359]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can see that if we take too few steps , we did not reach to the optimal value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets increase the learning rate $\\eta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44586015.81860581 [ 0.64081645 -2.22987593 -3.62215937]\n",
      "7069311510.619205 [ -1.55890545 -25.50453518 -31.61929409]\n",
      "1310662687403.5015 [ -31.94735733 -373.07612181 -382.41012641]\n",
      "243000004275404.0 [ -446.16053115 -5106.53474914 -5158.00179928]\n",
      "4.505278400043583e+16 [ -6086.62344389 -69558.51956322 -70183.63330206]\n",
      "8.352894282453912e+18 [ -82889.02983719 -947153.1618916  -955589.19365729]\n",
      "1.5486466473016364e+21 [ -1128650.59893636 -12896705.57457836 -13011497.05765287]\n",
      "2.871228052336887e+23 [-1.53680072e+07 -1.75604858e+08 -1.77167813e+08]\n",
      "5.323325719840954e+25 [-2.09254718e+08 -2.39108055e+09 -2.41236207e+09]\n",
      "9.869573646877738e+27 [-2.84926561e+09 -3.25575626e+10 -3.28473370e+10]\n",
      "1.8298426415668165e+30 [-3.87963271e+10 -4.43312074e+11 -4.47257716e+11]\n",
      "3.392572174539155e+32 [-5.28260683e+11 -6.03625024e+12 -6.08997511e+12]\n",
      "6.289910234905311e+34 [-7.19293217e+12 -8.21911225e+13 -8.29226540e+13]\n",
      "1.1661644536284863e+37 [-9.79407985e+13 -1.11913528e+15 -1.12909600e+15]\n",
      "2.162096885516361e+39 [-1.33358689e+15 -1.52384314e+16 -1.53740591e+16]\n",
      "4.008579517077942e+41 [-1.81584591e+16 -2.07490367e+17 -2.09337109e+17]\n",
      "7.432002632434888e+43 [-2.47250210e+17 -2.82524175e+18 -2.85038745e+18]\n",
      "1.3779111252053306e+46 [-3.36662190e+18 -3.84692121e+19 -3.88116023e+19]\n",
      "2.55468029663841e+48 [-4.58407823e+19 -5.23806601e+20 -5.28468674e+20]\n",
      "4.736438583482656e+50 [-6.24179780e+20 -7.13228425e+21 -7.19576422e+21]\n",
      "8.781470810505292e+52 [-8.49899105e+21 -9.71150010e+22 -9.79793605e+22]\n",
      "1.6281057642059607e+55 [-1.15724429e+23 -1.32234262e+24 -1.33411196e+24]\n",
      "3.0185471621332535e+57 [-1.57573334e+24 -1.80053543e+25 -1.81656086e+25]\n",
      "5.596458885130554e+59 [-2.14555869e+25 -2.45165495e+26 -2.47347559e+26]\n",
      "1.0375969090647627e+62 [-2.92144743e+26 -3.33823589e+27 -3.36794743e+27]\n",
      "1.9237295722143624e+64 [-3.97791731e+27 -4.54542712e+28 -4.58588310e+28]\n",
      "3.5666407972896826e+66 [-5.41643365e+28 -6.18916947e+29 -6.24425537e+29]\n",
      "6.612637639212674e+68 [-7.37515417e+29 -8.42733097e+30 -8.50233734e+30]\n",
      "1.2259988889478482e+71 [-1.00421980e+31 -1.14748687e+32 -1.15769993e+32]\n",
      "2.2730313646527302e+73 [-1.36737128e+32 -1.56244739e+33 -1.57635375e+33]\n",
      "4.214254703875875e+75 [-1.86184761e+33 -2.12746820e+34 -2.14640346e+34]\n",
      "7.813329365058367e+77 [-2.53513918e+34 -2.89681495e+35 -2.92259768e+35]\n",
      "1.4486100166356143e+80 [-3.45191015e+35 -3.94437711e+36 -3.97948353e+36]\n",
      "2.685757738156426e+82 [-4.70020888e+36 -5.37076445e+37 -5.41856625e+37]\n",
      "4.9794593059765995e+84 [-6.39992426e+37 -7.31296983e+38 -7.37805797e+38]\n",
      "9.232037062619389e+86 [-8.71430007e+38 -9.95752619e+39 -1.00461519e+40]\n",
      "1.7116418287278694e+89 [-1.18656132e+40 -1.35584216e+41 -1.36790966e+41]\n",
      "3.173425030661263e+91 [-1.61565215e+41 -1.84614925e+42 -1.86258067e+42]\n",
      "5.883606170522353e+93 [-2.19991316e+42 -2.51376390e+43 -2.53613733e+43]\n",
      "1.0908347049432408e+96 [-2.99545786e+43 -3.42280503e+44 -3.45326926e+44]\n",
      "2.022433723504921e+98 [-4.07869180e+44 -4.66057860e+45 -4.70205948e+45]\n",
      "3.7496406627279164e+100 [-5.55365076e+45 -6.34596267e+46 -6.40244409e+46]\n",
      "6.951923781817032e+102 [-7.56199249e+46 -8.64082459e+47 -8.71773113e+47]\n",
      "1.288903354089214e+105 [-1.02966018e+48 -1.17655671e+49 -1.18702850e+49]\n",
      "2.3896577527612366e+107 [-1.40201156e+49 -1.60202962e+50 -1.61628828e+50]\n",
      "4.430482826516577e+109 [-1.90901469e+50 -2.18136438e+51 -2.20077933e+51]\n",
      "8.214221494009724e+111 [-2.59936308e+51 -2.97020136e+52 -2.99663725e+52]\n",
      "1.5229363795029499e+114 [-3.53935905e+52 -4.04430191e+53 -4.08029770e+53]\n",
      "2.823560598779741e+116 [-4.81928153e+53 -5.50682461e+54 -5.55583739e+54]\n",
      "5.234949116904956e+118 [-6.56205662e+54 -7.49823280e+55 -7.56496986e+55]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([6.88163892e+55, 7.86340834e+56, 7.93339559e+56])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lr(y,x,.01,500,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that because of high learning rate , change is parameter is huge and we end up missing the optimal point , cost function values , as well as parameter values ended up exploding. Now lets run with low learning rate and higher number of steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18356616.836679872 [ 0.3967177  -0.00595057  0.04184493]\n",
      "25653.253792419913 [ 0.48118684  2.13054596 -3.86747094]\n",
      "23257.450244518728 [ 0.63566199  2.12387766 -3.87426105]\n",
      "21100.791927820854 [ 0.78222479  2.11753513 -3.88068772]\n",
      "19159.407484943178 [ 0.9212805   2.11151747 -3.88678522]\n",
      "17411.808653053013 [ 1.05321365  2.10580805 -3.89257039]\n",
      "15838.652090256626 [ 1.17838907  2.10039106 -3.89805924]\n",
      "14422.525273329902 [ 1.29715288  2.09525154 -3.90326695]\n",
      "13147.753766764683 [ 1.40983351  2.09037527 -3.90820792]\n",
      "12000.227729882454 [ 1.51674253  2.08574876 -3.9128958 ]\n",
      "10967.245741705841 [ 1.61817557  2.08135923 -3.91734357]\n",
      "10037.374214959103 [ 1.71441311  2.07719454 -3.92156352]\n",
      "9200.32084311801 [ 1.80572128  2.07324316 -3.92556731]\n",
      "8446.820679754801 [ 1.89235257  2.06949418 -3.92936603]\n",
      "7768.533589243868 [ 1.97454652  2.06593723 -3.93297018]\n",
      "7157.951933759943 [ 2.05253041  2.06256246 -3.93638972]\n",
      "6608.317474798442 [ 2.12651991  2.05936056 -3.9396341 ]\n",
      "6113.546569441836 [ 2.19671959  2.05632265 -3.94271231]\n",
      "5668.162833404114 [ 2.26332358  2.05344036 -3.94563285]\n",
      "5267.236525532439 [ 2.32651605  2.05070569 -3.94840379]\n",
      "4906.329982842767 [ 2.38647175  2.0481111  -3.9510328 ]\n",
      "4581.448502134508 [ 2.44335646  2.04564941 -3.95352716]\n",
      "4288.9961245174145 [ 2.49732748  2.0433138  -3.95589375]\n",
      "4025.7358334492665 [ 2.54853406  2.04109783 -3.95813912]\n",
      "3788.7537257365775 [ 2.59711778  2.03899536 -3.96026948]\n",
      "3575.42675892331 [ 2.64321301  2.03700058 -3.96229073]\n",
      "3383.3937180790144 [ 2.6869472   2.03510798 -3.96420844]\n",
      "3210.5290806317635 [ 2.72844128  2.03331232 -3.96602792]\n",
      "3054.919489967623 [ 2.76780999  2.03160863 -3.96775421]\n",
      "2914.8425773946356 [ 2.80516221  2.02999221 -3.96939208]\n",
      "2788.747898061306 [ 2.84060121  2.02845858 -3.97094606]\n",
      "2675.2397698186437 [ 2.87422499  2.02700351 -3.97242044]\n",
      "2573.0618250768575 [ 2.90612653  2.02562296 -3.97381929]\n",
      "2481.083104668953 [ 2.93639404  2.02431313 -3.9751465 ]\n",
      "2398.2855398006054 [ 2.96511122  2.02307039 -3.97640573]\n",
      "2323.752683529853 [ 2.99235749  2.0218913  -3.97760046]\n",
      "2256.6595670509864 [ 3.01820817  2.02077261 -3.97873399]\n",
      "2196.263568506525 [ 3.04273476  2.01971122 -3.97980947]\n",
      "2141.896193258218 [ 3.06600508  2.01870419 -3.98082985]\n",
      "2092.955674637371 [ 3.08808346  2.01774875 -3.98179798]\n",
      "2048.900313275191 [ 3.10903098  2.01684224 -3.98271651]\n",
      "2009.242481289617 [ 3.12890554  2.01598217 -3.983588  ]\n",
      "1973.5432249637897 [ 3.14776211  2.01516615 -3.98441484]\n",
      "1941.4074061756612 [ 3.16565282  2.01439192 -3.98519934]\n",
      "1912.4793288018182 [ 3.18262716  2.01365736 -3.98594365]\n",
      "1886.4388016857918 [ 3.19873206  2.01296041 -3.98664984]\n",
      "1862.9975945942654 [ 3.21401205  2.01229917 -3.98731985]\n",
      "1841.8962479333745 [ 3.22850938  2.0116718  -3.98795555]\n",
      "1822.901200913607 [ 3.24226415  2.01107656 -3.98855869]\n",
      "1805.8022063763697 [ 3.25531438  2.01051181 -3.98913093]\n",
      "1790.4100036679367 [ 3.26769618  2.00997598 -3.98967387]\n",
      "1776.5542238031308 [ 3.27944376  2.0094676  -3.99018899]\n",
      "1764.0815037318973 [ 3.29058962  2.00898526 -3.99067773]\n",
      "1752.8537888365047 [ 3.30116458  2.00852763 -3.99114143]\n",
      "1742.7468048705146 [ 3.31119788  2.00809344 -3.99158138]\n",
      "1733.6486824261428 [ 3.32071727  2.00768149 -3.9919988 ]\n",
      "1725.4587187048894 [ 3.32974906  2.00729063 -3.99239484]\n",
      "1718.0862628860618 [ 3.33831824  2.0069198  -3.99277059]\n",
      "1711.4497127558586 [ 3.3464485   2.00656796 -3.9931271 ]\n",
      "1705.475611491235 [ 3.35416232  2.00623415 -3.99346534]\n",
      "1700.0978346011582 [ 3.36148103  2.00591743 -3.99378626]\n",
      "1695.2568580260486 [ 3.36842486  2.00561693 -3.99409075]\n",
      "1690.8990992941763 [ 3.37501303  2.00533183 -3.99437963]\n",
      "1686.9763244427447 [ 3.38126375  2.00506133 -3.99465372]\n",
      "1683.4451141390446 [ 3.38719429  2.00480468 -3.99491377]\n",
      "1680.2663830924741 [ 3.39282108  2.00456118 -3.9951605 ]\n",
      "1677.4049474380067 [ 3.39815965  2.00433016 -3.99539459]\n",
      "1674.8291353026914 [ 3.40322477  2.00411096 -3.9956167 ]\n",
      "1672.5104362447282 [ 3.40803046  2.00390299 -3.99582742]\n",
      "1670.4231856849162 [ 3.41258999  2.00370568 -3.99602736]\n",
      "1668.5442808376229 [ 3.41691598  2.00351847 -3.99621705]\n",
      "1666.8529249969938 [ 3.42102039  2.00334085 -3.99639702]\n",
      "1665.330397348113 [ 3.42491456  2.00317233 -3.99656778]\n",
      "1663.9598457551622 [ 3.42860928  2.00301244 -3.99672979]\n",
      "1662.7261002331625 [ 3.43211474  2.00286074 -3.9968835 ]\n",
      "1661.615505038582 [ 3.43544066  2.00271681 -3.99702934]\n",
      "1660.6157675204022 [ 3.43859621  2.00258026 -3.99716771]\n",
      "1659.715822058607 [ 3.44159014  2.00245069 -3.99729899]\n",
      "1658.9057075840815 [ 3.44443072  2.00232777 -3.99742355]\n",
      "1658.1764573242604 [ 3.44712579  2.00221114 -3.99754173]\n",
      "1657.5199995541911 [ 3.44968283  2.00210048 -3.99765385]\n",
      "1656.9290682544333 [ 3.45210889  2.00199549 -3.99776023]\n",
      "1656.3971226869444 [ 3.45441068  2.00189588 -3.99786116]\n",
      "1655.9182749987463 [ 3.45659458  2.00180138 -3.99795692]\n",
      "1655.4872250520868 [ 3.45866661  2.00171171 -3.99804778]\n",
      "1655.0992017597223 [ 3.46063252  2.00162663 -3.99813399]\n",
      "1654.7499102760257 [ 3.46249772  2.00154592 -3.99821577]\n",
      "1654.4354844593672 [ 3.46426739  2.00146933 -3.99829337]\n",
      "1654.1524440796393 [ 3.46594642  2.00139667 -3.998367  ]\n",
      "1653.8976562972389 [ 3.46753944  2.00132773 -3.99843685]\n",
      "1653.6683009871465 [ 3.46905087  2.00126233 -3.99850312]\n",
      "1653.461839524311 [ 3.47048488  2.00120027 -3.998566  ]\n",
      "1653.2759866848041 [ 3.47184544  2.00114139 -3.99862566]\n",
      "1653.1086853517704 [ 3.47313631  2.00108553 -3.99868227]\n",
      "1652.9580837461692 [ 3.47436106  2.00103253 -3.99873597]\n",
      "1652.8225149303212 [ 3.47552308  2.00098224 -3.99878693]\n",
      "1652.7004783573552 [ 3.47662558  2.00093453 -3.99883527]\n",
      "1652.5906232623765 [ 3.47767161  2.00088926 -3.99888114]\n",
      "1652.491733711486 [ 3.47866406  2.00084632 -3.99892466]\n",
      "1652.4027151431867 [ 3.47960567  2.00080557 -3.99896594]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3.48049819,  2.00076694, -3.99900508])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_lr(y,x,.0004,100000,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that we ended up getting pretty good estimates for $\\beta$s , as good as from sklearn ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.497047463340305, 2.0000507703755974, -3.999730754941359]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sk_estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " there are modifications to gradient descent which can achieve the same thing in much less number of iterations. We'll discuss that in detail when we start with our course in Deep learning. For now ,we'll conclude this module "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
